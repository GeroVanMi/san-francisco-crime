{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## San Francisco Crime Prediction\n",
    "### Machine Learning I - ZHAW SoE - DS21a\n",
    "\n",
    "Gérôme Meyer, Lea Keller, Alessio Drigatti"
   ],
   "metadata": {
    "collapsed": false,
    "id": "HMlC7oa6XX1Q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this assignment we joined a kaggle challenge from 2014. Kaggle provides a dataset with nearly 12 years of crime reports from across all of San Francisco's neighborhoods. The dataset provides information about the exact time, the category of the incident, a description of the incident, the day of week, the name of de police department district, how the incident was resolved, approximate street address, geographical longitude and geographical latitude.\n",
    "The goal is to predict the category of an incident, given time and location."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "4VkZtsKMXX1S"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "id": "PnuTegBYXX1U"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data exploration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# There seems to be invalid data that contains latitude = 90. (Which would be the North Pole)\n",
    "data = data[data['Y'] != 90]\n",
    "data = data[data['Category'] != 'NONE']\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "x = np.asarray(data['X']).reshape((-1, 1))\n",
    "y = np.asarray(data['Y']).reshape((-1, 1))\n",
    "data['X'] = StandardScaler().fit_transform(x)\n",
    "data['Y'] = StandardScaler().fit_transform(y)\n",
    "\n",
    "sns.set(rc={'figure.figsize': (20, 15)})\n",
    "sns.scatterplot(data, x='Y', y='X', hue='Category')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "4LkIa5TtXX1V"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data cleaning and feature engineering\n",
    "First, we imported the data and deleted data with invalid latitude. Then we dropped the variables, which are not part of the test data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# There seems to be invalid data that contains latitude = 90. (Which would be the North Pole)\n",
    "data = data[data['Y'] != 90]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = data['Category']\n",
    "\n",
    "# The description and category are not part of the test data, therefore we cannot use them for training.\n",
    "# And we need to drop the resolution variable since it is our target variable and also not part of the test data.\n",
    "x_train = data.drop(['Descript', 'Category', 'Resolution'], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we separated the date informations in the subcomponents year, month, day, hour and minute and saved them as floats. We added the category federal holiday and set the name based on the date. We have also added two additional categories \"isday\" and \"IsWeekend\". The categorisation if it is day or night is based on the hour (time) and the categorisation if it is a weekday or the weekend is based on the day of week.\n",
    "After the new categories were added the information of the full date is no longer needed, so we dropped it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the date into its subcomponents\n",
    "x_train[['year', 'month', 'day', 'hour', 'minute']] = x_train.Dates.str.extract(\n",
    "    '(?P<year>\\d{4})-(?P<month>\\d{2})-(?P<day>\\d{2}) (?P<hour>\\d{2}):(?P<minute>\\d{2})')\n",
    "\n",
    "# Convert the date numbers into actual integers\n",
    "x_train = x_train.astype({\n",
    "    'year': 'float64',\n",
    "    'month': 'float64',\n",
    "    'day': 'float64',\n",
    "    'hour': 'float64',\n",
    "    'minute': 'float64',\n",
    "})\n",
    "\n",
    "x_train[\"federal_holiday\"] = \"None\"\n",
    "x_train.loc[x_train['Dates'].str.match('\\d{4}-01-01'), 'federal_holiday'] = 'new_year'\n",
    "x_train.loc[x_train['Dates'].str.match('\\d{4}-07-04'), 'federal_holiday'] = 'independence_day'\n",
    "x_train.loc[x_train['Dates'].str.match('\\d{4}-11-24'), 'federal_holiday'] = 'thanksgiving'\n",
    "x_train.loc[x_train['Dates'].str.match('\\d{4}-12-25'), 'federal_holiday'] = 'christmas_day'\n",
    "x_train.loc[x_train['Dates'].str.match('\\d{4}-12-26'), 'federal_holiday'] = 'christmas_day'\n",
    "x_train.loc[x_train['Dates'].str.match('\\d{4}-12-31'), 'federal_holiday'] = 'new_year'\n",
    "\n",
    "x_train.loc[x_train['hour']<=7,'isday']= 0\n",
    "x_train.loc[x_train['hour']>7,'isday']= 1\n",
    "x_train.loc[x_train['hour']>19,'isday']= 0\n",
    "\n",
    "x_train[\"IsWeekend\"]= 0\n",
    "x_train.loc[x_train['DayOfWeek'] == 'Sunday', 'IsWeekend'] = 1\n",
    "x_train.loc[x_train['DayOfWeek'] == 'Saturday', 'IsWeekend'] = 1\n",
    "\n",
    "x_train.drop(['Dates'], inplace=True, axis=1)"
   ],
   "metadata": {
    "id": "17WtEAI7XX1X"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO Gérôme"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# One-Hot encoding for the DayOfTheWeek\n",
    "column_transformer = make_column_transformer(\n",
    "    (OneHotEncoder(), ['DayOfWeek', 'PdDistrict']),\n",
    "    (StandardScaler(), ['X', 'Y', 'year', 'month', 'day', 'hour', 'minute']),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "x_train_transformed = column_transformer.fit_transform(x_train)\n",
    "x_train = pd.DataFrame(data=x_train_transformed, columns=column_transformer.get_feature_names_out())\n",
    "\n",
    "x_train = x_train.rename(columns={element: re.sub(r'^(.+)__', '', element) for element in x_train.columns.tolist()})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train.to_csv('../data/x_train_cleaned.csv', index=False)\n",
    "y_train.to_csv('../data/y_train_cleaned.csv', index=False)\n",
    "\n",
    "x_train.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Implementation\n",
    "\n",
    "In order to try and predict the category we trained a fully-connected sequential Neural Network on the training data from Kaggle.\n",
    "\n",
    "\n",
    "Architecture:\n",
    "- Input Layer with a Neuron for each feature\n",
    "- Hidden Layer with **2560** Neurons\n",
    "- Hidden Layer with **780** Neurons\n",
    "- Output Layer with **39** Categories\n",
    "\n",
    "The output of the hidden Layers is normalized with a \"[BatchNormalization](https://keras.io/api/layers/normalization_layers/batch_normalization/)\" Layer.\n",
    "\n",
    "\n",
    "Note: The code below contains a commented section that can be uncommented to work with only a fraction of the data which is useful for trying out different architecures.\n",
    "\n",
    "First we import the following libraries:\n",
    "- matplotlib to plot the score of the model\n",
    "- pandas to read and transform our data that we created earlier\n",
    "- kears for the actual Neural Network\n",
    "- sklearn (Scikit-Learn) for some more data preprocessing and splitting\n",
    "- numpy for when we wish to reduce"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we read the data from the files created earlier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "x_clean = pd.read_csv('../data/x_train_cleaned.csv')\n",
    "y_clean = pd.read_csv('../data/y_train_cleaned.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For architecture experimentation you can run this cell with a custom \"data_fraction\".\n",
    "E.g. with data_fraction = 2, you would work with only half of the data, therefore reducing training time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "data_fraction = 20\n",
    "drop_indices = np.random.choice(x_clean.index, int(len(x_clean) / data_fraction), replace=False)\n",
    "x_clean = x_clean.drop(drop_indices)\n",
    "y_clean = y_clean.drop(drop_indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# TODO: Deal with the Address. Maybe we can use it in a useful manner\n",
    "x_clean.drop(['Address'], inplace=True, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can split the data to into training and test data. We need the test part of the data to evaluate how well our model will deal with similar data that it hasn't seen yet.\n",
    "Also we encode the string descriptions of the category into numbers since our model cannot be trained to predict strings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_clean = label_encoder.fit_transform(y_clean['Category'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_clean, y_clean)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model definition\n",
    "\n",
    "Creation of the model and its"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32866/35188 [===========================>..] - ETA: 14s - loss: 2.7048 - accuracy: 0.2066"
     ]
    }
   ],
   "source": [
    "model = Sequential(name='san_francisco_sequential')\n",
    "model.add(Input(shape=x_train.shape[1]))\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Dense(2560, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(780, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output Layer for 39 categories\n",
    "model.add(Dense(39, activation='softmax'))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    validation_split=.1\n",
    ")\n",
    "\n",
    "model.save('./small_model')\n",
    "\n",
    "for j in list(history.history.keys()):\n",
    "    plt.plot(history.history[j])\n",
    "    plt.title(j + ' over epochs')\n",
    "    plt.ylabel(j)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.show()\n",
    "\n",
    "print(model.evaluate(x_test, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/test.csv')\n",
    "\n",
    "# TODO: What are we supposed to do to get predictions for the data sets that do not have proper coordinates?\n",
    "data = data[data['Y'] != 90]\n",
    "\n",
    "# There seems to be invalid data that contains latitude = 90. (Which would be the North Pole)\n",
    "standard_scaler = StandardScaler()\n",
    "x = np.asarray(data['X']).reshape((-1, 1))\n",
    "y = np.asarray(data['Y']).reshape((-1, 1))\n",
    "data['X'] = StandardScaler().fit_transform(x)\n",
    "data['Y'] = StandardScaler().fit_transform(y)\n",
    "\n",
    "print(data['Y'].max())\n",
    "\n",
    "sns.set(rc={'figure.figsize': (20, 15)})\n",
    "sns.scatterplot(data, x='Y', y='X')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "nfnCog4pXX1a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "1G0yQpCWXX1b"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/test.csv')\n",
    "\n",
    "# There seems to be invalid data that contains latitude = 90. (Which would be the North Pole)\n",
    "# data = data[data['Y'] != 90]\n",
    "\n",
    "\n",
    "# The description and category are not part of the test data, therefore we cannot use them for training.\n",
    "# And we need to drop the resolution variable since it is our target variable and also not part of the test data.\n",
    "\n",
    "# TODO: Extract this into a function. ###\n",
    "# Split the date into its subcomponents\n",
    "data[['year', 'month', 'day', 'hour', 'minute']] = data.Dates.str.extract(\n",
    "    '(?P<year>\\d{4})-(?P<month>\\d{2})-(?P<day>\\d{2}) (?P<hour>\\d{2}):(?P<minute>\\d{2})')\n",
    "\n",
    "# data[\"federal_holiday\"] = \"None\"\n",
    "# data.loc[data['Dates'].str.match('\\d{4}-01-01'), 'federal_holiday'] = 'new_year'\n",
    "# data.loc[data['Dates'].str.match('\\d{4}-07-04'), 'federal_holiday'] = 'independence_day'\n",
    "# data.loc[data['Dates'].str.match('\\d{4}-11-24'), 'federal_holiday'] = 'thanksgiving'\n",
    "# data.loc[data['Dates'].str.match('\\d{4}-12-25'), 'federal_holiday'] = 'christmas_day'\n",
    "# data.loc[data['Dates'].str.match('\\d{4}-12-26'), 'federal_holiday'] = 'christmas_day'\n",
    "# data.loc[data['Dates'].str.match('\\d{4}-12-31'), 'federal_holiday'] = 'new_year'\n",
    "\n",
    "data.drop(['Dates'], inplace=True, axis=1)\n",
    "\n",
    "# Convert the date numbers into actual integers\n",
    "data = data.astype({\n",
    "    'year': 'float64',\n",
    "    'month': 'float64',\n",
    "    'day': 'float64',\n",
    "    'hour': 'float64',\n",
    "    'minute': 'float64',\n",
    "})\n",
    "\n",
    "# One-Hot encoding for the DayOfTheWeek\n",
    "column_transformer = make_column_transformer(\n",
    "    (OneHotEncoder(), ['DayOfWeek', 'PdDistrict']),\n",
    "    (StandardScaler(), ['X', 'Y', 'year', 'month', 'day', 'hour', 'minute']),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "data_transformed = column_transformer.fit_transform(data)\n",
    "data = pd.DataFrame(data=data_transformed, columns=column_transformer.get_feature_names_out())\n",
    "\n",
    "data = data.rename(columns={element: re.sub(r'^(.+)__', '', element) for element in data.columns.tolist()})\n",
    "\n",
    "data.to_csv('../data/test_cleaned.csv', index=False)\n",
    "\n",
    "data"
   ],
   "metadata": {
    "id": "XzIvDr2jXX1c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Kaggle submission"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "\n",
    "categories = ['ARSON', 'ASSAULT', 'BAD CHECKS', 'BRIBERY', 'BURGLARY', 'DISORDERLY CONDUCT',\n",
    "              'DRIVING UNDER THE INFLUENCE', 'DRUG/NARCOTIC', 'DRUNKENNESS', 'EMBEZZLEMENT', 'EXTORTION',\n",
    "              'FAMILY OFFENSES', 'FORGERY/COUNTERFEITING', 'FRAUD', 'GAMBLING', 'KIDNAPPING', 'LARCENY/THEFT',\n",
    "              'LIQUOR LAWS', 'LOITERING', 'MISSING PERSON', 'NON-CRIMINAL', 'OTHER OFFENSES', 'PORNOGRAPHY/OBSCENE MAT',\n",
    "              'PROSTITUTION', 'RECOVERED VEHICLE', 'ROBBERY', 'RUNAWAY', 'SECONDARY CODES', 'SEX OFFENSES FORCIBLE',\n",
    "              'SEX OFFENSES NON FORCIBLE', 'STOLEN PROPERTY', 'SUICIDE', 'SUSPICIOUS OCC', 'TREA', 'TRESPASS',\n",
    "              'VANDALISM', 'VEHICLE THEFT', 'WARRANTS', 'WEAPON LAWS']\n",
    "\n",
    "model = keras.models.load_model('./small_model')\n",
    "df = pd.read_csv('../data/test_cleaned.csv')\n",
    "\n",
    "df.drop(['Address'], inplace=True, axis=1)\n",
    "df.drop(['Id'], inplace=True, axis=1)\n",
    "\n",
    "arr = df.to_numpy()\n",
    "\n",
    "predictions = model.predict(arr)\n",
    "\n",
    "for prediction in predictions:\n",
    "    prediction_max = prediction.max()\n",
    "    prediction[prediction != prediction_max] = 0\n",
    "    prediction[prediction != 0] = 1\n",
    "\n",
    "print(predictions.shape)\n",
    "\n",
    "predictions = predictions.astype('int32')\n",
    "\n",
    "df_predictions = pd.DataFrame(predictions, columns=categories)\n",
    "\n",
    "df_predictions.to_csv('predictions.csv', index_label='Id')"
   ],
   "metadata": {
    "id": "20xHrlbLXX1d"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
